
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>1. Surrogates &#8212; eXplainable&lt;br&gt;Machine Learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/xmlx.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <link rel="canonical" href="https://book.xmlx.io/book/meta_explainers/surrogates/index.html" />
    <link rel="shortcut icon" href="../../../_static/bulb.svg"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="1.1. Overview" href="intro.html" />
    <link rel="prev" title="Meta-Explainers" href="../index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6635EH38S9"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-6635EH38S9');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/bulb.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">eXplainable<br>Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../README.html">
                    eXplainable Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../preface/index.html">
   Preface
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../preface/glossary.html">
     Glossary
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../preface/preliminary.html">
     Preliminary Information
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../preface/data.html">
     Modules, Data Sets and Models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../index.html">
   Meta-Explainers
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active has-children">
    <a class="current reference internal" href="#">
     1. Surrogates
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="intro.html">
       1.1. Overview
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="image.html">
       1.2. Image Surrogates
      </a>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="tabular/index.html">
       1.3. Tabular Surrogates
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
      <label for="toctree-checkbox-4">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="tabular/interpretable_representation.html">
         1.3.1. Binary Interpretable Representations for Tabular Data
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="tabular/data_sampling.html">
         1.3.2. Data Sampling
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="tabular/explanation_generation.html">
         1.3.3. Explanation Generation
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="examples/index.html">
       1.4. Interactive Examples
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
      <label for="toctree-checkbox-5">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="examples/ex_tabular.html">
         1.4.1. Surrogate Explainer of Tabular Data
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="examples/ex_ols.html">
         1.4.2. Investigating Linear Surrogate Explainers of Tabular Data
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../bibliography/bibliography.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../genindex.html">
   Index
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://book.xmlx.io/docs/">
   XML Book Documentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://xmlx.io/">
   XMLX Homepage
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            <a href="https://github.com/xmlx-io/xml-book/blob/master/LICENCE"><img src="https://img.shields.io/badge/licence-CC%20BY--NC--SA%204.0-lightgrey" alt="Licence"></a> </br> <a href="https://doi.org/XX.XXXX/zenodo.XXXXXXX"><img src="https://zenodo.org/badge/DOI/XX.XXXX/zenodo.XXXXXXX.svg" alt="DOI"></a>

            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/xmlx-io/xml-book/master?urlpath=lab/tree/book/meta_explainers/surrogates/index.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/xmlx-io/xml-book/blob/master/book/meta_explainers/surrogates/index.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/xmlx-io/xml-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/xmlx-io/xml-book/issues/new?title=Issue%20on%20page%20%2Fbook/meta_explainers/surrogates/index.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/xmlx-io/xml-book/edit/master/book/meta_explainers/surrogates/index.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/book/meta_explainers/surrogates/index.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download notebook file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        <a href="../../../_sources/book/meta_explainers/surrogates/index.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Surrogates</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="surrogates">
<span id="text-meta-explainers-surrogates"></span><h1><span class="section-number">1. </span>Surrogates<a class="headerlink" href="#surrogates" title="Permalink to this headline">#</a></h1>
<div class="important dropdown admonition">
<p class="admonition-title">Explainer Summary</p>
<p>Surrogate explainers construct an inherently interpretable model to approximate a more complex, opaque decision boundary in a desired subspace.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><em>Property</em></p></th>
<th class="head"><p><strong>Surrogate Explainers</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><em>relation</em></p></td>
<td><p>post-hoc</p></td>
</tr>
<tr class="row-odd"><td><p><em>compatibility</em></p></td>
<td><p>model-agnostic / (semi-)supervised</p></td>
</tr>
<tr class="row-even"><td><p><em>modelling</em></p></td>
<td><p>regression, crisp and probabilistic classification</p></td>
</tr>
<tr class="row-odd"><td><p><em>scope</em></p></td>
<td><p>local, cohort or global</p></td>
</tr>
<tr class="row-even"><td><p><em>target</em></p></td>
<td><p>prediction or model</p></td>
</tr>
<tr class="row-odd"><td><p><em>data</em></p></td>
<td><p>text, image or tabular (data-universal)</p></td>
</tr>
<tr class="row-even"><td><p><em>features</em></p></td>
<td><p>numerical and categorical (tabular data)</p></td>
</tr>
<tr class="row-odd"><td><p><em>explanation</em></p></td>
<td><p>type depends on the surrogate model</p></td>
</tr>
<tr class="row-even"><td><p><em>caveats</em></p></td>
<td><p>random sampling, explanation faithfulness</p></td>
</tr>
</tbody>
</table>
</div>
<p>Surrogate explainers construct an inherently interpretable model in a desired – local, cohort or global – subspace to approximate a more complex, black-box decision boundary <span id="id1">[<a class="reference internal" href="../../bibliography/bibliography.html#id5" title="Mark Craven and Jude W Shavlik. Extracting tree-structured representations of trained networks. In Advances in neural information processing systems, 24–30. 1996.">Craven and Shavlik, 1996</a>; <a class="reference internal" href="../../bibliography/bibliography.html#id12" title="Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “Why should I trust you?”: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, 1135–1144. 2016.">Ribeiro <em>et al.</em>, 2016</a>; <a class="reference internal" href="../../bibliography/bibliography.html#id16" title="Kacper Sokol, Alexander Hepburn, Raul Santos-Rodriguez, and Peter Flach. bLIMEy: Surrogate prediction explanations beyond LIME. 2019 Workshop on Human-Centric Machine Learning (HCML 2019) at the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada, 2019. arXiv preprint arXiv:1910.13016. URL: https://arxiv.org/abs/1910.13016.">Sokol <em>et al.</em>, 2019</a>]</span>.
An intuitive visualisation of this process for tabular data is shown in <a class="reference internal" href="#fig-surrogates-overview"><span class="std std-numref">Figure 1.1</span></a>, where the red and blue areas capture two different classes determined by two continuous features.
While in this example the surrogate is a <em>linear</em> model fitted <em>locally</em> within a specific neighbourhood, different classifiers can be used to generate other types of explanations – while linear models offer feature influence <span id="id2">[<a class="reference internal" href="../../bibliography/bibliography.html#id12" title="Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “Why should I trust you?”: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, 1135–1144. 2016.">Ribeiro <em>et al.</em>, 2016</a>]</span>, decision trees, for example, can provide feature importance and counterfactuals <span id="id3">[<a class="reference internal" href="../../bibliography/bibliography.html#id2" title="Kacper Sokol and Peter Flach. LIMEtree: Interactively customisable explanations based on local surrogate multi-output regression trees. arXiv preprint arXiv:2005.01427, 2020. URL: https://arxiv.org/abs/2005.01427.">Sokol and Flach, 2020</a>; <a class="reference internal" href="../../bibliography/bibliography.html#id13" title="Jasper van der Waa, Marcel Robeer, Jurriaan van Diggelen, Matthieu Brinkhuis, and Mark Neerincx. Contrastive explanations with local foil trees. Workshop on Human Interpretability in Machine Learning (WHI 2018) at the 35th International Conference on Machine Learning (ICML 2018), Stockholm, Sweden, 2018. arXiv preprint arXiv:1806.07470. URL: https://arxiv.org/abs/1806.07470.">van der Waa <em>et al.</em>, 2018</a>]</span>.
Furthermore, this simplistic depiction of surrogates should not be taken at its face value as these explainers often rely on custom <em><a class="reference internal" href="../../preface/glossary.html#term-interpretable-representation"><span class="xref std std-term">interpretable representations</span></a></em> – which transform raw features into human-intelligible concepts <span id="id4">[<a class="reference internal" href="../../bibliography/bibliography.html#id3" title="Kacper Sokol and Peter Flach. Towards faithful and meaningful interpretable representations. arXiv preprint arXiv:2008.07007, 2020. URL: http://arxiv.org/abs/2008.07007.">Sokol and Flach, 2020</a>]</span> – in which case understanding their behaviour as well as properties and limitations of the resulting explanations require more nuance.
If poorly fitted, the surrogate model may output untrustworthy (and possibly misleading) insights, therefore its <em><a class="reference internal" href="../../preface/glossary.html#term-fidelity"><span class="xref std std-term">fidelity</span></a></em> – which captures the quality of black-box decision boundary approximation – should be evaluated and maximised <span id="id5">[<a class="reference internal" href="../../bibliography/bibliography.html#id15" title="Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):206–215, 2019.">Rudin, 2019</a>]</span>.
For the example shown in <a class="reference internal" href="#fig-surrogates-overview"><span class="std std-numref">Figure 1.1</span></a> this property is determined by the alignment between the dashed line (surrogate model) and the fragment of the red–blue decision boundary of the black box chosen to be explained.</p>
<figure class="align-default" id="fig-surrogates-overview" style="width: 95%">
<div class="cell_output docutils container">
<img alt="../../../_images/index_2_0.svg" src="../../../_images/index_2_0.svg" /></div>
<figcaption>
<p><span class="caption-number">Figure 1.1 </span><span class="caption-text">A visualisation of a <em>local</em>, <em>linear</em> surrogate model – the black, dashed line – for two-dimensional tabular data with two classes.
The explanation is generated by extracting feature coefficients from this model.</span><a class="headerlink" href="#fig-surrogates-overview" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<!-- :alt: "Local, linear surrogate explainer for two-dimensional tabular data." -->
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>While <a class="reference internal" href="#fig-surrogates-overview"><span class="std std-numref">Figure 1.1</span></a> offers a good idea of surrogate explainers, this intuition may be misleading when dealing with their more complex instantiations.
For example, if they are built atop an <a class="reference internal" href="../../preface/glossary.html#term-interpretable-representation"><span class="xref std std-term">interpretable representation</span></a>, they do not <em>directly</em> approximate a black-box decision surface but instead capture its behaviour through the prism of concepts encoded by the interpretable representation.</p>
</div>
<h2> Overview of Surrogate Explainers </h2>
<p>Surrogates are an attractive explainability approach as they are <em>post-hoc</em> and <em>model-agnostic</em>, i.e., they can be retrofitted into a preëxisting black-box model regardless of its type.
Additionally, they can be applied to a range of diverse modelling problems across regression as well as crisp and probabilistic classification tasks.
Surrogate explainers are also <em>data-universal</em> due to the use of interpretable representations, therefore they work with text, image and tabular (with both numerical and categorical features) data.
For text and tabular data their local, cohort or global scope can be easily adjusted; however, for images they are predominantly local, i.e., valid only with respect to a single image.
This flexibility allows them to target an individual prediction, a group thereof, or scale up all the way to mimicking, hence simplifying, an entire black-box model.</p>
<p>A wide selection of transparent models that can be used as surrogates enables these explainers to provide a broad range of explanation types.
For example, decision trees can offer <a class="reference internal" href="../../preface/glossary.html#term-counterfactual"><span class="xref std std-term">counterfactuals</span></a> and <a class="reference internal" href="../../preface/glossary.html#term-feature-importance"><span class="xref std std-term">feature importance</span></a> <span id="id6">[<a class="reference internal" href="../../bibliography/bibliography.html#id2" title="Kacper Sokol and Peter Flach. LIMEtree: Interactively customisable explanations based on local surrogate multi-output regression trees. arXiv preprint arXiv:2005.01427, 2020. URL: https://arxiv.org/abs/2005.01427.">Sokol and Flach, 2020</a>; <a class="reference internal" href="../../bibliography/bibliography.html#id13" title="Jasper van der Waa, Marcel Robeer, Jurriaan van Diggelen, Matthieu Brinkhuis, and Mark Neerincx. Contrastive explanations with local foil trees. Workshop on Human Interpretability in Machine Learning (WHI 2018) at the 35th International Conference on Machine Learning (ICML 2018), Stockholm, Sweden, 2018. arXiv preprint arXiv:1806.07470. URL: https://arxiv.org/abs/1806.07470.">van der Waa <em>et al.</em>, 2018</a>]</span>, whereas linear classifiers deliver <a class="reference internal" href="../../preface/glossary.html#term-feature-influence"><span class="xref std std-term">feature influence</span></a> <span id="id7">[<a class="reference internal" href="../../bibliography/bibliography.html#id12" title="Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “Why should I trust you?”: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, 1135–1144. 2016.">Ribeiro <em>et al.</em>, 2016</a>]</span>.
While this diversity may be alluring, it comes at a cost.
Since the explanations are extracted from a surrogate model, they may not be truthful with respect to the underlying black box, i.e., exhibit <strong>poor</strong> <em>fidelity</em>.
Even when we explicitly optimise for faithfulness, it may be impossible to guarantee that the surrogate explanations capture the exact characteristics of the black-box decision boundary.
Additionally, surrogates rely on (<strong>random</strong>) <em>data sampling</em> to discover the behaviour of the explained model in a selected neighbourhood.</p>
<p>LIME – Local Interpretable Model-agnostic Explanations <span id="id8">[<a class="reference internal" href="../../bibliography/bibliography.html#id12" title="Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “Why should I trust you?”: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, 1135–1144. 2016.">Ribeiro <em>et al.</em>, 2016</a>]</span> – is one of the most popular surrogate explainers but it is just a single instantiation of their much broader family.
To cover the breadth and depth of this explainer type, this chapter is based on the bLIMEy – build LIME yourself <span id="id9">[<a class="reference internal" href="../../bibliography/bibliography.html#id16" title="Kacper Sokol, Alexander Hepburn, Raul Santos-Rodriguez, and Peter Flach. bLIMEy: Surrogate prediction explanations beyond LIME. 2019 Workshop on Human-Centric Machine Learning (HCML 2019) at the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada, 2019. arXiv preprint arXiv:1910.13016. URL: https://arxiv.org/abs/1910.13016.">Sokol <em>et al.</em>, 2019</a>]</span> – meta-algorithm.
It provides a generalisation of LIME into a flexible framework that supports building bespoke surrogate explainers that are appropriate for the problem at hand and exhibit the desired properties.
bLIMEy has three algorithmic steps.</p>
<ol class="simple">
<li><p><em>Interpretable representation composition.</em>
If desired, data are transformed from their original domain into a human-intelligible representation, which is used to convey the explanations.
For example, <a class="reference internal" href="../../preface/glossary.html#term-super-pixel"><span class="xref std std-term">super-pixel</span></a> segmentation of images; <a class="reference internal" href="../../preface/glossary.html#term-bag-of-words"><span class="xref std std-term">bag-of-words</span></a> representation of text; and discretisation of continuous features for tabular data.
While this step is required for the former two domains, it is not necessary – albeit helpful – for the latter.</p></li>
<li><p><em>Data sampling.</em>
To capture the behaviour of a black box in a desired subspace, a data sample is generated and predicted by the explained model.</p></li>
<li><p><em>Explanation generation.</em>
Explanatory insights are extracted from an inherently transparent model fitted into the data sampled in the previous step, using their black-box predictions as the target.
Additional processing steps can be applied to tune and tweak the surrogate model, hence the explanation.
For example, the sample can be <em>weighted</em> based on its proximity to the explained instance when dealing with local explanations; and a feature selection procedure may be used to introduce sparsity, therefore improve accessibility and comprehensibility of explanatory insights.</p></li>
</ol>
<p>These steps are operationalised differently depending on the type of data, hence their algorithmic approaches are covered by separate sections of this chapter (see the table below).
A visual depiction of a step-by-step process of building a <em>local</em>, <em>linear</em> surrogate for two-dimensional <em>tabular data</em> <em>without an interpretable representation</em> is captured by <a class="reference internal" href="intro.html#fig-surrogates-full-overview"><span class="std std-numref">Figure 1.2</span></a>.</p>
<h2> High-level Summary </h2>
<p>Surrogate explainers may be a good solution, or should be avoided, depending on individual circumstances.</p>
<h3> Pros </h3>
<ul class="simple">
<li><p>A universal inspection mechanism for various subspaces of an arbitrary black-box algorithmic decision process.</p></li>
<li><p>Highly customisable.</p></li>
<li><p>A single explanatory procedure for image, text and tabular data.</p></li>
<li><p>Produces diverse explanation types depending on the utilised surrogate model.</p></li>
<li><p>Outputs intuitive explanations for image and text data due to the use of interpretable representations.</p></li>
</ul>
<h3> Cons </h3>
<ul class="simple">
<li><p>Inadequate for high-stakes algorithmic decisions because of lacklustre fidelity <span id="id10">[<a class="reference internal" href="../../bibliography/bibliography.html#id15" title="Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):206–215, 2019.">Rudin, 2019</a>]</span>.</p></li>
<li><p>Explanations may be counterintuitive and misleading for a lay audience when applied to tabular data <em>with</em> an interpretable representation.</p></li>
<li><p>While post-hoc, model-agnostic and data-universal, they must not be treated as a silver bullet.</p></li>
<li><p>Their characteristics allow a single instantiation of a surrogate explainer to be applied to diverse problems, however the quality of the resulting explanations will vary across different problems and data sets.</p></li>
<li><p>Building them requires an effort since each explainer should be tweaked and tuned to the problem at hand.</p></li>
</ul>
<h2> Popular Surrogate Explainers </h2>
<p>The following approaches are popular in the space of surrogate explainers.</p>
<ul class="simple">
<li><p>LIME – Local Interpretable Model-agnostic Explanations <span id="id11">[<a class="reference internal" href="../../bibliography/bibliography.html#id12" title="Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “Why should I trust you?”: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, 1135–1144. 2016.">Ribeiro <em>et al.</em>, 2016</a>]</span> – is a <em>local</em> surrogate explainer based on a <em>linear model</em>. Its explanations capture the influence of human-intelligible components, which are determined by the underlying interpretable representation, on a selected black-box prediction. It works with text, image and tabular data.</p></li>
<li><p>bLIMEy – build LIME yourself <span id="id12">[<a class="reference internal" href="../../bibliography/bibliography.html#id16" title="Kacper Sokol, Alexander Hepburn, Raul Santos-Rodriguez, and Peter Flach. bLIMEy: Surrogate prediction explanations beyond LIME. 2019 Workshop on Human-Centric Machine Learning (HCML 2019) at the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada, 2019. arXiv preprint arXiv:1910.13016. URL: https://arxiv.org/abs/1910.13016.">Sokol <em>et al.</em>, 2019</a>]</span> – is a surrogate meta-algorithm that provides a generic framework for building bespoke surrogate explainers.</p></li>
</ul>
<p><a class="reference internal" href="intro.html#text-meta-explainers-surrogates-overview-literature"><span class="std std-numref">Section 1.1.6</span></a> offers a broader overview of relevant literature.</p>
<h2> Implementations </h2>
<p>The following software packages implement surrogate explainers.
Note that many packages only offer an interface to the official LIME implementation; these instances are marked with the <span class="far fa-lemon"></span> symbol.</p>
<table class="table">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><span class="fab fa-python"></span> Python</p></th>
<th class="head"><p><span class="fab fa-r-project"></span> R</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/marcotcr/lime">LIME</a></p></td>
<td><p><a class="reference external" href="https://github.com/thomasp85/lime">lime</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/interpretml/interpret">interpret</a> <span class="far fa-lemon"></span></p></td>
<td><p><a class="reference external" href="https://github.com/christophM/iml">iml</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/oracle/Skater">Skater</a> <span class="far fa-lemon"></span></p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/Trusted-AI/AIX360">AIX360</a> <span class="far fa-lemon"></span></p></td>
<td></td>
</tr>
</tbody>
</table>
<h2> Contents of This Chapter </h2>
<p>This chapter is split into sections dedicated to different types of data, which are further subdivided into three components of surrogate explainers.
Specifically, image and tabular data are discussed.</p>
<table class="table">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Images</p></th>
<th class="head"><p>Tabular</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="image.html#text-meta-explainers-surrogates-image"><span class="std std-numref">Overview (§1.2)</span></a></p></td>
<td><p><a class="reference internal" href="tabular/index.html#text-meta-explainers-surrogates-tabular"><span class="std std-numref">Overview (§1.3)</span></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="image.html#text-meta-explainers-surrogates-image-interpretable-representation"><span class="std std-numref">Interpretable Representation (§1.2.1)</span></a></p></td>
<td><p><a class="reference internal" href="tabular/interpretable_representation.html#text-meta-explainers-surrogates-tabular-interpretable-representation"><span class="std std-numref">Interpretable Representation (§1.3.1)</span></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="image.html#text-meta-explainers-surrogates-image-data-sampling"><span class="std std-numref">Data Sampling (§1.2.2)</span></a></p></td>
<td><p><a class="reference internal" href="tabular/data_sampling.html#text-meta-explainers-surrogates-tabular-data-sampling"><span class="std std-numref">Data Sampling (§1.3.2)</span></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="image.html#text-meta-explainers-surrogates-image-explanation-generation"><span class="std std-numref">Explanation Generation (§1.2.3)</span></a></p></td>
<td><p><a class="reference internal" href="tabular/explanation_generation.html#text-meta-explainers-surrogates-tabular-explanation-generation"><span class="std std-numref">Explanation Generation (§1.3.3)</span></a></p></td>
</tr>
</tbody>
</table>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./book/meta_explainers/surrogates"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../index.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Meta-Explainers</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="intro.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">1.1. </span>Overview</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By <a href="mailto:kacper@xmlx.io">Kacper Sokol</a> and <a href="mailto:team@xmlx.io">XMLX Team</a> &ndash; <a href="https://xmlx.io/">xmlx.io</a>.<br> Distributed under <a href="https://github.com/xmlx-io/xml-book/blob/master/LICENCE">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Licence</a>.
<br/>
  
      &copy; Copyright 2021–2022.<br/>
    <div class="extra_footer">
      <p> This book delivers a comprehensive outlook on the most popular explainability concepts in Machine Learning. It covers a range of theoretical and practical topics across different difficulty levels, including but not limited to: high-level overviews & introductory examples; mathematical foundations; algorithmic implementations; practical advice & real-life caveats; and success & failure case studies. </p>

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>