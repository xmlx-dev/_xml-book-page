
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>1.1. Overview &#8212; eXplainable&lt;br&gt;Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/xmlx.css?v=cffff2e3" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=9d383507"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-6635EH38S9"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-6635EH38S9');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-6635EH38S9');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"IR": "\\mathit{IR}", "argmin": "\\mathop{\\operatorname{arg\\,min}}\\limits", "argmax": "\\mathop{\\operatorname{arg\\,max}}\\limits"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'book/meta_explainers/surrogates/intro';</script>
    <link rel="canonical" href="https://book.xmlx.dev/book/meta_explainers/surrogates/intro.html" />
    <link rel="icon" href="../../../_static/bulb.svg"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="1.2. Text Surrogates" href="text.html" />
    <link rel="prev" title="1. Surrogates" href="index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/bulb.svg" class="logo__image only-light" alt="eXplainable<br>Machine Learning - Home"/>
    <script>document.write(`<img src="../../../_static/bulb.svg" class="logo__image only-dark" alt="eXplainable<br>Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../README.html">
                    eXplainable Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../preface/index.html">Preface</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../preface/glossary.html">Glossary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../preface/preliminary.html">Preliminary Information</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../preface/data.html">Modules, Data Sets and Models</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../index.html">Meta-Explainers</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active has-children"><a class="reference internal" href="index.html">1. Surrogates</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3 current active"><a class="current reference internal" href="#">1.1. Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="text.html">1.2. Text Surrogates</a></li>
<li class="toctree-l3"><a class="reference internal" href="image.html">1.3. Image Surrogates</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="tabular/index.html">1.4. Tabular Surrogates</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="tabular/interpretable_representation.html">1.4.1. Binary Interpretable Representations for Tabular Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="tabular/data_sampling.html">1.4.2. Data Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="tabular/explanation_generation.html">1.4.3. Explanation Generation</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="examples/index.html">1.5. Interactive Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="examples/ex_tabular.html">1.5.1. Surrogate Explainer of Tabular Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="examples/ex_ols.html">1.5.2. Investigating Linear Surrogate Explainers of Tabular Data</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../bibliography/bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../genindex.html">Index</a></li>
<li class="toctree-l1"><a class="reference external" href="https://book.xmlx.dev/docs/">XML Book Documentation</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xmlx.dev/">XMLX Homepage</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/xmlx-dev/xml-book/master?urlpath=lab/tree/book/meta_explainers/surrogates/intro.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/xmlx-dev/xml-book/blob/master/book/meta_explainers/surrogates/intro.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/xmlx-dev/xml-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/xmlx-dev/xml-book/edit/master/book/meta_explainers/surrogates/intro.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/xmlx-dev/xml-book/issues/new?title=Issue%20on%20page%20%2Fbook/meta_explainers/surrogates/intro.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/book/meta_explainers/surrogates/intro.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../../../_sources/book/meta_explainers/surrogates/intro.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Overview</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-surrogates">1.1.1. Building Surrogates</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimisation-objective">Optimisation Objective</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#complexity">Complexity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fidelity-loss-one-class">Fidelity Loss (One Class)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fidelity-loss-multiple-classes">Fidelity Loss (Multiple Classes)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fidelity-loss-scaling">Fidelity Loss Scaling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#surrogate-building-blocks">1.1.2. Surrogate Building Blocks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretable-representations">Interpretable Representations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-sampling">Data Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-generation">Explanation Generation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#surrogates-across-data-types">1.1.3. Surrogates Across Data Types</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text">Text</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#images">Images</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tabular-data">Tabular Data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-examples">1.1.4. Explanation Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear">Linear</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tree">Tree</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-output-tree">Multi-output Tree</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-strategies">1.1.5. Evaluation Strategies</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Interpretable Representations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Data Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Explanation Generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#end-to-end">End-to-End</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#caveats">1.1.6. Caveats</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">1.1.7. Additional Resources</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="overview">
<span id="text-meta-explainers-surrogates-overview"></span><h1><span class="section-number">1.1. </span>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h1>
<p>To better understand surrogate explainers, we discuss them on multiple levels: abstract, theoretical and algorithmic, for image, text and tabular data.
Next, we overview explanations that can be generated with surrogate trees and linear models; introduce evaluation criteria and strategies; go through caveats of surrogate explainers; and present relevant literature.
You can navigate through these sections using the right-hand side <em>Contents</em> panel.</p>
<div class="note dropdown admonition">
<p class="admonition-title">Mathematical Notation Summary</p>
<div class="pst-scrollable-table-container"><table class="table" id="text-meta-explainers-surrogates-maths">
<thead>
<tr class="row-odd"><th class="head"><p>Symbol</p></th>
<th class="head"><p>Meaning</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathcal{X}\)</span></p></td>
<td><p>Original representation of data.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\mathcal{X}^\prime\)</span></p></td>
<td><p>Interpretable representation of data.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(X\)</span></p></td>
<td><p>A subset of data in the original representation <span class="math notranslate nohighlight">\(X \subseteq \mathcal{X}\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(X^\prime\)</span></p></td>
<td><p>A subset of data in the interpretable representation <span class="math notranslate nohighlight">\(X^\prime \subseteq \mathcal{X}^\prime\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(x\)</span></p></td>
<td><p>A data point in the original representation <span class="math notranslate nohighlight">\(x \in \mathcal{X}\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(x^\prime\)</span></p></td>
<td><p>A data point in the interpretable representation <span class="math notranslate nohighlight">\(x^\prime \in \mathcal{X}^\prime\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathring{x}\)</span></p></td>
<td><p>A data point selected to be explained in the original representation <span class="math notranslate nohighlight">\(\mathring{x} \in \mathcal{X}\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\mathring{x}^\prime\)</span></p></td>
<td><p>A data point selected to be explained in the interpretable representation <span class="math notranslate nohighlight">\(\mathring{x}^\prime \in \mathcal{X}^\prime\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathcal{Y}\)</span></p></td>
<td><p>The label space of the data <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>.
For crisp classification it encodes unique classes <span class="math notranslate nohighlight">\(c \in \mathcal{Y}\)</span>;
for probabilistic classification <span class="math notranslate nohighlight">\(\mathcal{Y} \;\colon= \left[ 0, 1 \right]^C\)</span>, where <span class="math notranslate nohighlight">\(C\)</span> is the set of all classes and <span class="math notranslate nohighlight">\(c \in C\)</span> denotes an individual class; and
for regression it is a numerical range, e.g., <span class="math notranslate nohighlight">\(\mathcal{Y} \equiv \mathbb{R}\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(Y\)</span></p></td>
<td><p>Labels <span class="math notranslate nohighlight">\(Y \subseteq \mathcal{Y}\)</span> for a subset of data <span class="math notranslate nohighlight">\(X \subseteq \mathcal{X}\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathring{C}\)</span></p></td>
<td><p>A subset of classes <span class="math notranslate nohighlight">\(\mathring{C} \subseteq \mathcal{Y}\)</span> selected to be explained.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\mathring{c}\)</span></p></td>
<td><p>A class <span class="math notranslate nohighlight">\(\mathring{c} \in \mathcal{Y}\)</span> selected to be explained.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(d\)</span></p></td>
<td><p>Dimensionality – i.e., number of features – of the data space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(d^\prime\)</span></p></td>
<td><p>Dimensionality – i.e., number of features – of the <em>interpretable</em> data space <span class="math notranslate nohighlight">\(\mathcal{X}^\prime\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathcal{F} : \mathcal{X} \mapsto \mathcal{Y}\)</span></p></td>
<td><p>A family of black-box models compatible with the explainer.
If the model family represents probabilistic or crisp classifiers, a subscript <span class="math notranslate nohighlight">\(c\)</span> – i.e., <span class="math notranslate nohighlight">\(f_c \in \mathcal{F}\)</span> – is used to indicate that it outputs: a probability of class <span class="math notranslate nohighlight">\(c\)</span> for the former type; and for the latter <span class="math notranslate nohighlight">\(1\)</span> if the predicted class is <span class="math notranslate nohighlight">\(c\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise (refer to <a class="reference internal" href="#equation-eq-meta-explainers-surrogates-crisp-classification-target">Equation 1.5</a>).</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(f \in \mathcal{F}\)</span></p></td>
<td><p>A black-box model being investigated.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathcal{G} : \mathcal{X}\)</span> or <span class="math notranslate nohighlight">\(\mathcal{X}^\prime \mapsto [0, 1]\)</span> or <span class="math notranslate nohighlight">\(\{0, 1\}\)</span> or <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span></p></td>
<td><p>A family of inherently transparent and human-comprehensible models that can be used as surrogates.
These models can be fitted to the original <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> (specific to tabular) or interpretable <span class="math notranslate nohighlight">\(\mathcal{X}^\prime\)</span> (tabular, image and text) data representation.
They are trained to model the probability of a selected class (<span class="math notranslate nohighlight">\([0, 1]\)</span>) for probabilistic classifiers, the <span class="math notranslate nohighlight">\(\{0, 1\}\)</span> encoding of a selected class for crisp models, and a numerical output, e.g., <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>, for regressors.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(g \in \mathcal{G}\)</span></p></td>
<td><p>A surrogate model used for explanation generation.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\IR : \mathcal{X} \mapsto \mathcal{X}^\prime\)</span></p></td>
<td><p>A (user-provided) transformation function from the original into the interpretable domain.
The inverse transformation is dented with <span class="math notranslate nohighlight">\(\IR^{-1}\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(L : \mathcal{X} \times \mathcal{X} \mapsto \mathbb{R}\)</span> or <br> <span class="math notranslate nohighlight">\(L : \mathcal{X}^\prime \times \mathcal{X}^\prime \mapsto \mathbb{R}\)</span></p></td>
<td><p>A distance metric compatible with either the original <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> or interpretable <span class="math notranslate nohighlight">\(\mathcal{X}^\prime\)</span> data domain.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(k : \mathbb{R} \mapsto \mathbb{R}\)</span></p></td>
<td><p>A kernel transforming distances into similarity scores.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\omega : \mathcal{X} \times \mathcal{X} \mapsto \mathbb{R}\)</span> or <br> <span class="math notranslate nohighlight">\(\omega : \mathcal{X}^\prime \times \mathcal{X}^\prime \mapsto \mathbb{R}\)</span></p></td>
<td><p>A weighting function that captures the proximity or similarity of an instance to the explained data point, operating either in the original <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> or interpretable <span class="math notranslate nohighlight">\(\mathcal{X}^\prime\)</span> data domain.
It is usually defined as <span class="math notranslate nohighlight">\(\omega = k \circ L\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\Omega : \mathcal{G} \mapsto \mathbb{R}\)</span></p></td>
<td><p>A function that measures complexity of a surrogate model as perceived by a human <a class="reference internal" href="../../preface/glossary.html#term-explainee"><span class="xref std std-term">explainee</span></a>; for example, the number of non-zero (or significantly larger than zero) parameters of a linear model, or a size (depth or width) of a decision tree.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\mathcal{L} : \mathcal{F} \times \mathcal{G} \mapsto \mathbb{R}\)</span></p></td>
<td><p>A function that calculates the quality of a surrogate model by assessing how well it mimics the predictions of the explained black box (also known as <a class="reference internal" href="../../preface/glossary.html#term-fidelity"><span class="xref std std-term">fidelity</span></a>).</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathcal{O}\)</span></p></td>
<td><p>The optimisation objective used to construct surrogate models.
See <a class="reference internal" href="#equation-eq-meta-explainers-surrogates-optimisation">Equation 1.1</a> for the definition.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\({\Large\mathbb{1}} : \mathbb{R} \mapsto \{0, 1\}\)</span></p></td>
<td><p>An indicator function for a specific condition, e.g.,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
     {\Large\mathbb{1}}\left(x\right) =
     \begin{cases}
       1, &amp; \text{if} \;\; x &gt; 0\\
       0, &amp; \text{otherwise}
     \end{cases}
     \text{ .}
    \end{split}\]</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<section id="building-surrogates">
<h2><span class="section-number">1.1.1. </span>Building Surrogates<a class="headerlink" href="#building-surrogates" title="Link to this heading">#</a></h2>
<p>The introduction to <a class="reference internal" href="index.html#text-meta-explainers-surrogates"><span class="std std-numref">Chapter 1</span></a> provided a high-level overview of surrogate explainers.
It also outlined the three fundamental components of the <a class="reference internal" href="../../preface/glossary.html#term-bLIMEy"><span class="xref std std-term">bLIMEy</span></a> framework:</p>
<ul class="simple">
<li><p>interpretable data representation composition (step 1),</p></li>
<li><p>data sampling (steps 2–4) and</p></li>
<li><p>explanation generation (steps 5–8),</p></li>
</ul>
<p>which can be codified in a <em>meta-algorithm</em> as shown by <a class="reference internal" href="#algo:meta-explainers:surrogates:blimey">Algorithm 1.1</a>, with the <em>steps</em> corresponding to each module listed in parentheses.
In this process, <span class="math notranslate nohighlight">\(\IR : \mathcal{X} \mapsto \mathcal{X}^\prime\)</span> is a function transforming data from their original domain <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> into the interpretable representation <span class="math notranslate nohighlight">\(\mathcal{X}^\prime\)</span>;
<span class="math notranslate nohighlight">\(X^\prime \subseteq \mathcal{X}^\prime\)</span> are data sampled in the interpretable representations and transformed to the original <span class="math notranslate nohighlight">\(\IR^{-1}(X^\prime) = X \subseteq \mathcal{X}\)</span>;
<span class="math notranslate nohighlight">\(f \in \mathcal{F} : \mathcal{X} \mapsto \mathcal{Y}\)</span> is the black-box model being explained, with a <span class="math notranslate nohighlight">\(\mathring{c}\)</span> subscript – i.e., <span class="math notranslate nohighlight">\(f_\mathring{c}\)</span> – indicating the user-selected class to be predicted (and explained); and
<span class="math notranslate nohighlight">\(\omega : \mathcal{X}^\prime \times \mathcal{X}^\prime \mapsto \mathbb{R}\)</span> is a function measuring similarity between two instances.
Note that sampling (step 2) and similarity calculation (step 5) are done in the interpretable representation <span class="math notranslate nohighlight">\(\mathcal{X}^\prime\)</span>, but these operations can also be performed in the original domain <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> (with certain caveats).
Additionally, the order of steps 5 and 6 – which are <em>optional</em> – can be inverted.
Refer to the <em><a class="reference internal" href="#text-meta-explainers-surrogates-maths"><span class="std std-ref">Mathematical Notation Summary</span></a></em> box at the top of this page for an overview of mathematical notation used throughout this chapter.</p>
<div class="proof algorithm admonition" id="algo:meta-explainers:surrogates:blimey">
<p class="admonition-title"><span class="caption-number">Algorithm 1.1 </span> (bLIMEy meta-algorithm)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs:</strong> an instance to be explained <span class="math notranslate nohighlight">\(\mathring{x}\)</span>, a class <span class="math notranslate nohighlight">\(\mathring{c}\)</span> for which to explain this instance, and a black-box model <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p><strong>Output:</strong> an explanation <span class="math notranslate nohighlight">\(e\)</span>.</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathring{x}^\prime \gets \IR(\mathring{x})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(X^\prime \gets \mathtt{sample\_data}(\mathring{x}^\prime, \;\; \mathtt{scope}=\{\mathtt{local},\; \mathtt{cohort},\; \mathtt{global}\})\)</span> <em>(done in <span class="math notranslate nohighlight">\(\mathcal{X}^\prime\)</span>)</em></p></li>
<li><p><span class="math notranslate nohighlight">\(X \gets \IR^{-1}(X^\prime)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y \gets f_\mathring{c}(X)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(w \gets \omega(\mathring{x}^\prime, \; X^\prime)\)</span> <em>(get weights)</em></p></li>
<li><p><span class="math notranslate nohighlight">\(\overline{X}^\prime \gets \mathtt{reduce\_dimensionality}(X^\prime)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(g \gets \mathtt{fit\_surrogate}(\overline{X}^\prime, \; y, \; \mathtt{sample\_weight}=w)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(e \gets \mathtt{extract\_explanation}(g)\)</span></p></li>
</ol>
</section>
</div><p>A toy example of this meta-algorithm being executed for two-dimensional tabular data with numerical features is shown in <a class="reference internal" href="#fig-surrogates-full-overview"><span class="std std-numref">Figure 1.2</span></a>.
In this particular case we do not use an interpretable representation to offer more clarity and a better intuition of the process.
Therefore, the representation transformations are not needed (steps 1 and 3);
the data are sampled in their original domain <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> (step 2);
the similarity is computed in the original domain <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> as well (step 5);
the dimensionality reduction is not needed (step 6); and
the surrogate is fitted onto the data sample in the original domain <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> (step 7).</p>
<figure class="align-default" id="fig-surrogates-full-overview" style="width: 95%">
<img alt="../../../_images/00c68478f5c917e26bf8b0cf6b085f71fed2203f8f73cec7f052722fb99f9494.svg" src="../../../_images/00c68478f5c917e26bf8b0cf6b085f71fed2203f8f73cec7f052722fb99f9494.svg" />
<figcaption>
<p><span class="caption-number">Figure 1.2 </span><span class="caption-text">Steps required to build a <em>local</em>, <em>linear</em> surrogate explainer for two-dimensional, numerical <em>tabular data</em> without an interpretable representation.
Panel (a) shows an instance selected to be explained for the black box whose decision boundary is shown by the blue and red background shading.
Panel (b) depicts a collection of data sampled locally around the explained instance.
Panel (c) illustrates similarity between this data sample and the explained instance using distance-based weighting.
Finally, Panel (d) captures a surrogate linear model approximating the black-box decision boundary in this region.</span><a class="headerlink" href="#fig-surrogates-full-overview" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="optimisation-objective">
<h3>Optimisation Objective<a class="headerlink" href="#optimisation-objective" title="Link to this heading">#</a></h3>
<p>Formally, surrogates are built by optimising the objective <span class="math notranslate nohighlight">\(\mathcal{O}\)</span> given in <a class="reference internal" href="#equation-eq-meta-explainers-surrogates-optimisation">Equation 1.1</a>.
It balances the (human <a class="reference internal" href="../../preface/glossary.html#term-explainee"><span class="xref std std-term">explainee</span></a>-perceived) complexity <span class="math notranslate nohighlight">\(\Omega : \mathcal{G} \mapsto \mathbb{R}\)</span> of the surrogate model <span class="math notranslate nohighlight">\(g \in \mathcal{G}\)</span>, which is chosen from the space of a transparent family of models <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>, against this model’s ability to approximate the explained black box <span class="math notranslate nohighlight">\(\mathcal{L} : \mathcal{F} \times \mathcal{G} \mapsto \mathbb{R}\)</span>, where <span class="math notranslate nohighlight">\(f \in \mathcal{F}\)</span> is the opaque model drawn from a family of models <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> compatible with this explainer.
The surrogate model may be fitted to the original <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> or the interpretable <span class="math notranslate nohighlight">\(\mathcal{X}^\prime\)</span> domain if one is used.
It can either target the probability of a selected class (<span class="math notranslate nohighlight">\([0, 1]\)</span>) for probabilistic classifiers <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>, the <span class="math notranslate nohighlight">\(\{0, 1\}\)</span> encoding of a selected class for crisp models <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>, or a numerical output, e.g., <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>, for regressors <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-meta-explainers-surrogates-optimisation">
<span class="eqno">(1.1)<a class="headerlink" href="#equation-eq-meta-explainers-surrogates-optimisation" title="Link to this equation">#</a></span>\[
  \mathcal{O}(\mathcal{G}; \; f) =
  \argmin_{g \in \mathcal{G}}
  \overbrace{\Omega(g)}^{\text{complexity}} \; + \;\;\;
  \overbrace{\mathcal{L}(f, g)}^{\text{fidelity loss}}
\]</div>
</section>
<section id="complexity">
<h3>Complexity<a class="headerlink" href="#complexity" title="Link to this heading">#</a></h3>
<p>The complexity <span class="math notranslate nohighlight">\(\Omega\)</span>, for example, can be the number of non-zero (or <em>substantially</em> larger than zero) parameters of a linear model (<a class="reference internal" href="#equation-eq-meta-explainers-surrogates-complexity-linear">Equation 1.2</a>, where <span class="math notranslate nohighlight">\(\Theta_g\)</span> are the coefficients of the surrogate and <span class="math notranslate nohighlight">\({\Large\mathbb{1}} : \mathbb{R} \mapsto \{0, 1\}\)</span> is an indicator function), or the size – depth or width – of a decision tree (<a class="reference internal" href="#equation-eq-meta-explainers-surrogates-complexity-tree">Equation 1.3</a>, where <span class="math notranslate nohighlight">\(d\)</span> is the dimensionality of the data space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>).</p>
<div class="math notranslate nohighlight" id="equation-eq-meta-explainers-surrogates-complexity-linear">
<span class="eqno">(1.2)<a class="headerlink" href="#equation-eq-meta-explainers-surrogates-complexity-linear" title="Link to this equation">#</a></span>\[
  \Omega(g) = \frac{\sum_{\theta \in \Theta_g} {\Large\mathbb{1}} \left(\theta\right)}{|\Theta_g|}
\]</div>
<div class="math notranslate nohighlight" id="equation-eq-meta-explainers-surrogates-complexity-tree">
<span class="eqno">(1.3)<a class="headerlink" href="#equation-eq-meta-explainers-surrogates-complexity-tree" title="Link to this equation">#</a></span>\[
  \Omega(g; \; d) = \frac{\text{depth}(g)}{d}
  \;\;\;\;\text{or}\;\;\;\;
  \Omega(g; \; d) = \frac{\text{width}(g)}{2^d}
\]</div>
</section>
<section id="fidelity-loss-one-class">
<h3>Fidelity Loss (One Class)<a class="headerlink" href="#fidelity-loss-one-class" title="Link to this heading">#</a></h3>
<p>The second component of the optimisation objective <span class="math notranslate nohighlight">\(\mathcal{O}\)</span> – the <a class="reference internal" href="../../preface/glossary.html#term-fidelity"><span class="xref std std-term">fidelity</span></a> loss of the surrogate explainer <span class="math notranslate nohighlight">\(\mathcal{L} : \mathcal{F} \times \mathcal{G} \mapsto \mathbb{R}\)</span> – measures how well the surrogate model mimics the predictions of the explained black box (more specifically, its decision surface contained within the subspace of interest and determined by the data sampled in this region).</p>
<hr class="docutils" />
<p>The formulation of fidelity <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> depends on the type of the explained black box.
For <em>regressors</em> and <em>probabilistic classifiers</em> it is based on a weighted – determined by the proximity <span class="math notranslate nohighlight">\(\omega(\cdot, \cdot)\)</span> of each instance to the explained data point – squared error;
in the latter scenario the numerical optimisation uses the probability of a single class <span class="math notranslate nohighlight">\(\mathring{c}\)</span> selected by the user to be explained.
In both cases the surrogate model is a regressor.
<a class="reference internal" href="#equation-eq-meta-explainers-surrogates-loss-regression">Equation 1.4</a> defines the loss for these two cases, with the calculations performed on a data set <span class="math notranslate nohighlight">\(X^\prime\)</span> captured in the interpretable representation <span class="math notranslate nohighlight">\(\mathcal{X}^\prime\)</span>, the weighting <span class="math notranslate nohighlight">\(\omega(\cdot, \cdot)\)</span> also done in the interpretable space <span class="math notranslate nohighlight">\(\mathcal{X}^\prime\)</span>, and the surrogate model <span class="math notranslate nohighlight">\(g\)</span> operating on the interpretable domain <span class="math notranslate nohighlight">\(\mathcal{X}^\prime\)</span> as well.</p>
<div class="math notranslate nohighlight" id="equation-eq-meta-explainers-surrogates-loss-regression">
<span class="eqno">(1.4)<a class="headerlink" href="#equation-eq-meta-explainers-surrogates-loss-regression" title="Link to this equation">#</a></span>\[
  \mathcal{L}(f, g ; \; \mathring{x}, X^\prime, \mathring{c}) =
  \sum_{x^\prime \in X^\prime} \;
  \underbrace{\omega\left( \IR(\mathring{x}), x^\prime \right)}_{\text{weighting factor}}
  \; \times \;
  \underbrace{\left(f_\mathring{c}\left(\IR^{-1}(x^\prime)\right) - g(x^\prime)\right)^{2}}_{\text{individual loss}}
\]</div>
<hr class="docutils" />
<p>The loss <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> for <em>crisp classification</em> is inspired by predictive accuracy – see <a class="reference internal" href="#equation-eq-meta-explainers-surrogates-loss-classification">Equation 1.6</a>.
Here, the <em>individual loss</em> component (the underscored part of <a class="reference internal" href="#equation-eq-meta-explainers-surrogates-loss-classification">Equation 1.6</a>) is based on an indicator function whose role depends on whether the black box is a binary or multi-class classifier and how the local model is trained.
Specifically, after choosing a class to be explained <span class="math notranslate nohighlight">\(\mathring{c}\)</span>, we encode the output of the explained model <span class="math notranslate nohighlight">\(f\)</span> as <span class="math notranslate nohighlight">\(1\)</span> if it is <span class="math notranslate nohighlight">\(\mathring{c}\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise, i.e.:</p>
<div class="math notranslate nohighlight" id="equation-eq-meta-explainers-surrogates-crisp-classification-target">
<span class="eqno">(1.5)<a class="headerlink" href="#equation-eq-meta-explainers-surrogates-crisp-classification-target" title="Link to this equation">#</a></span>\[\begin{split}
f_{\mathring{c}}(x) =
\begin{cases}
  1, &amp; \text{if} \;\; f(x) \equiv \mathring{c}\\
  0, &amp; \text{if} \;\; f(x) \not\equiv \mathring{c}
\end{cases} \text{ .}
\end{split}\]</div>
<p>This representation is used as target when training the surrogate model, which in this case is a <em>crisp binary classifier</em>.
Then, the indicator function in <a class="reference internal" href="#equation-eq-meta-explainers-surrogates-loss-classification">Equation 1.6</a> becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
{\Large\mathbb{1}}\left(f_{\mathring{c}}(x), g(x^\prime)\right) =
\begin{cases}
  1, &amp; \text{if} \;\; f_{\mathring{c}}(x) \equiv g(x^\prime)\\
  0, &amp; \text{if} \;\; f_{\mathring{c}}(x) \not\equiv g(x^\prime)
\end{cases} \text{ ,}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(x^\prime = \IR(x)\)</span>.
For example, for three classes <span class="math notranslate nohighlight">\(\mathcal{Y} \colon= \{\alpha, \beta, \gamma\}\)</span>, <span class="math notranslate nohighlight">\(\mathring{c} \equiv \beta\)</span> and some surrogate <span class="math notranslate nohighlight">\(g\)</span>:</p>
<div class="pst-scrollable-table-container"><table class="centre-table table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(f(x)\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(f_\beta(x)\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(g(x^\prime)\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\({\Large\mathbb{1}}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\alpha\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\beta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\gamma\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\beta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\alpha\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<div class="math notranslate nohighlight" id="equation-eq-meta-explainers-surrogates-loss-classification">
<span class="eqno">(1.6)<a class="headerlink" href="#equation-eq-meta-explainers-surrogates-loss-classification" title="Link to this equation">#</a></span>\[
  \mathcal{L}(f, g ; \; \mathring{x}, X^\prime, \mathring{c}) =
  \sum_{x^\prime \in X^\prime} \;
  \omega\left( \IR(\mathring{x}), x^\prime \right)
  \; \times \;
  \underline{ {\Large\mathbb{1}} \left(f_\mathring{c}\left(\IR^{-1}(x^\prime)\right), \; g(x^\prime)\right)}
\]</div>
<hr class="docutils" />
<p>The <em>weighting</em> function <span class="math notranslate nohighlight">\(\omega\)</span> captures the proximity or similarity between the explained instance (<span class="math notranslate nohighlight">\(\mathring{x}\)</span>) and the data sampled in its neighbourhood (<span class="math notranslate nohighlight">\(X^\prime\)</span>).
It is usually defined as <span class="math notranslate nohighlight">\(\omega = k \circ L\)</span>, where <span class="math notranslate nohighlight">\(L : \mathcal{X}^\prime \times \mathcal{X}^\prime \mapsto \mathbb{R}\)</span> is a distance metric and <span class="math notranslate nohighlight">\(k : \mathbb{R} \mapsto \mathbb{R}\)</span> is a kernel transforming distances into similarity scores – see <a class="reference internal" href="#equation-eq-meta-explainers-surrogates-weighting-interpretable">Equation 1.7</a>.
Here we assume that the distance function operates on the interpretable representation <span class="math notranslate nohighlight">\(\mathcal{X}^\prime\)</span> but it may as well be applied to the original data domain <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>, i.e., <span class="math notranslate nohighlight">\(L : \mathcal{X} \times \mathcal{X} \mapsto \mathbb{R}\)</span> – see <a class="reference internal" href="#equation-eq-meta-explainers-surrogates-weighting-original">Equation 1.8</a>.
Since the surrogate is explicitly local, the weights are computed in reference to the explained instance, however if the <em>scope</em> is broadened beyond a single instance, different weighting strategies should be considered.</p>
<div class="math notranslate nohighlight" id="equation-eq-meta-explainers-surrogates-weighting-interpretable">
<span class="eqno">(1.7)<a class="headerlink" href="#equation-eq-meta-explainers-surrogates-weighting-interpretable" title="Link to this equation">#</a></span>\[
\omega\left(\IR(\mathring{x}), x^\prime \right) = k\left(L\left(\IR(\mathring{x}), x^\prime\right)\right)
\]</div>
<div class="math notranslate nohighlight" id="equation-eq-meta-explainers-surrogates-weighting-original">
<span class="eqno">(1.8)<a class="headerlink" href="#equation-eq-meta-explainers-surrogates-weighting-original" title="Link to this equation">#</a></span>\[
\omega\left( \mathring{x}, x \right) = k\left(L\left(\mathring{x}, x\right)\right)
\]</div>
</section>
<section id="fidelity-loss-multiple-classes">
<h3>Fidelity Loss (Multiple Classes)<a class="headerlink" href="#fidelity-loss-multiple-classes" title="Link to this heading">#</a></h3>
<p>In certain scenarios it may be beneficial to explain multiple classes at the same time, i.e., by a single surrogate model, in which case the surrogate becomes a multi-output predictor <span id="id1">[<a class="reference internal" href="../../bibliography/bibliography.html#id2" title="Kacper Sokol and Peter Flach. LIMEtree: Interactively customisable explanations based on local surrogate multi-output regression trees. arXiv preprint arXiv:2005.01427, 2020. URL: https://arxiv.org/abs/2005.01427.">Sokol and Flach, 2020</a>]</span>.
Such a setting allows to capture more complex behaviour of the explained black box, for example, <a class="reference internal" href="#text-meta-explainers-surrogates-overview-explanations-multi-output-trees"><span class="std std-ref">consistent interactions between the classes of interest</span></a>.
Operationalising multi-output surrogates requires adapting the <a class="reference internal" href="../../preface/glossary.html#term-fidelity"><span class="xref std std-term">fidelity</span></a> loss <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> to account for the black box and surrogate predicting more than one class – see <a class="reference internal" href="#equation-eq-meta-explainers-surrogates-loss-multi-output-regression">Equation 1.9</a> for probabilistic and regression black boxes and <a class="reference internal" href="#equation-eq-meta-explainers-surrogates-loss-multi-output-classification">Equation 1.10</a> for black-box crisp classifiers, where <span class="math notranslate nohighlight">\(\mathring{C}\)</span> is the set of user-defined classes to be explained.
In this formalisation, <a class="reference internal" href="#equation-eq-meta-explainers-surrogates-loss-multi-output-regression">Equation 1.9</a> requires a <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span> scaling factor to ensure that the sum over <span class="math notranslate nohighlight">\(\mathring{C}\)</span> is in the <span class="math notranslate nohighlight">\([0, 1]\)</span> range<a class="footnote-reference brackets" href="#scaling" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, but this scaling factor should be discarded for black-box regressors;
<a class="reference internal" href="#equation-eq-meta-explainers-surrogates-loss-multi-output-classification">Equation 1.10</a> is scaled by <span class="math notranslate nohighlight">\(\frac{1}{|\mathring{C}|}\)</span> for the same reason.</p>
<div class="math notranslate nohighlight" id="equation-eq-meta-explainers-surrogates-loss-multi-output-regression">
<span class="eqno">(1.9)<a class="headerlink" href="#equation-eq-meta-explainers-surrogates-loss-multi-output-regression" title="Link to this equation">#</a></span>\[
  \mathcal{L}(f, g ; \; \mathring{x}, X^\prime, \mathring{C}) =
  \sum_{x^\prime \in X^\prime}
  %\left(
    \omega( \IR(\mathring{x}) , x^\prime )
    \; \times \;
    \underline{
      \frac{1}{2}
      \sum_{\mathring{c} \in \mathring{C}}
      \left(
          f_\mathring{c}\left(\IR^{-1}(x^\prime)\right) -
          g_\mathring{c}(x^\prime)
      \right)^2
    }
  %\right)
\]</div>
<div class="math notranslate nohighlight" id="equation-eq-meta-explainers-surrogates-loss-multi-output-classification">
<span class="eqno">(1.10)<a class="headerlink" href="#equation-eq-meta-explainers-surrogates-loss-multi-output-classification" title="Link to this equation">#</a></span>\[
  \mathcal{L}(f, g ; \; \mathring{x}, X^\prime, \mathring{C}) =
  \sum_{x^\prime \in X^\prime}
  %\left(
    \omega( \IR(\mathring{x}) , x^\prime )
    \; \times \;
    \underline{
      \frac{1}{|\mathring{C}|}
      \sum_{\mathring{c} \in \mathring{C}}
      {\Large\mathbb{1}}
      \left(
        f_\mathring{c}\left(\IR^{-1}(x^\prime)\right), \;
        g_\mathring{c}(x^\prime)
      \right)
    }
  %\right)
\]</div>
</section>
<section id="fidelity-loss-scaling">
<h3>Fidelity Loss Scaling<a class="headerlink" href="#fidelity-loss-scaling" title="Link to this heading">#</a></h3>
<p>All of the defined <a class="reference internal" href="../../preface/glossary.html#term-fidelity"><span class="xref std std-term">fidelity</span></a> loss functions <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> –
<a class="reference internal" href="#equation-eq-meta-explainers-surrogates-loss-regression">Equation 1.4</a>,
<a class="reference internal" href="#equation-eq-meta-explainers-surrogates-loss-classification">Equation 1.6</a>,
<a class="reference internal" href="#equation-eq-meta-explainers-surrogates-loss-multi-output-regression">Equation 1.9</a> and
<a class="reference internal" href="#equation-eq-meta-explainers-surrogates-loss-multi-output-classification">Equation 1.10</a> –
are <em>unbounded</em>, which may make the loss values incomparable across different surrogates.
To abate this problem and make the optimisation easier you may want to consider scaling each loss by the factor given in <a class="reference internal" href="#equation-eq-meta-explainers-surrogates-loss-scaling">Equation 1.11</a>, thus bringing it to the <span class="math notranslate nohighlight">\([0, 1]\)</span> range.
Since weighting is <em>optional</em>, scaling the loss to the <span class="math notranslate nohighlight">\([0, 1]\)</span> range for <em>unweighted loss</em> can be achieved with the <span class="math notranslate nohighlight">\(\frac{1}{|X^\prime|}\)</span> factor.</p>
<div class="math notranslate nohighlight" id="equation-eq-meta-explainers-surrogates-loss-scaling">
<span class="eqno">(1.11)<a class="headerlink" href="#equation-eq-meta-explainers-surrogates-loss-scaling" title="Link to this equation">#</a></span>\[
  \frac{1}{\sum_{x^\prime \in X^\prime} \omega( \IR(\mathring{x}) , x^\prime )}
\]</div>
</section>
</section>
<section id="surrogate-building-blocks">
<span id="text-meta-explainers-surrogates-overview-surrogate-components"></span><h2><span class="section-number">1.1.2. </span>Surrogate Building Blocks<a class="headerlink" href="#surrogate-building-blocks" title="Link to this heading">#</a></h2>
<p>Surrogate explainers are built from three core components outlined by the bLIMEy (build LIME yourself) framework <span id="id3">[<a class="reference internal" href="../../bibliography/bibliography.html#id16" title="Kacper Sokol, Alexander Hepburn, Raul Santos-Rodriguez, and Peter Flach. bLIMEy: Surrogate prediction explanations beyond LIME. 2019 Workshop on Human-Centric Machine Learning (HCML 2019) at the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada, 2019. arXiv preprint arXiv:1910.13016. URL: https://arxiv.org/abs/1910.13016.">Sokol <em>et al.</em>, 2019</a>]</span>: <em>interpretable data representation composition</em>, <em>data sampling</em> and <em>explanation generation</em>.
Each one is responsible for, and influences, a different aspect of the resulting explanations.
Choosing, configuring and tweaking these building blocks is a non-trivial task that may require domain knowledge, understanding of the problem at hand, familiarity with the data being modelled or human-in-the-loop design.
These components are discussed in more details separately for each type of data: <a class="reference internal" href="text.html#text-meta-explainers-surrogates-text"><span class="std std-ref">text</span></a>, <a class="reference internal" href="image.html#text-meta-explainers-surrogates-image"><span class="std std-ref">image</span></a> and <a class="reference internal" href="tabular/index.html#text-meta-explainers-surrogates-tabular"><span class="std std-ref">tabular</span></a>.</p>
<section id="interpretable-representations">
<h3>Interpretable Representations<a class="headerlink" href="#interpretable-representations" title="Link to this heading">#</a></h3>
<p>Interpretable representations (IRs) facilitate translating the “language” of <a class="reference internal" href="../../preface/glossary.html#term-ML"><span class="xref std std-term">ML</span></a> models – i.e., low-level data representations required for good predictive performance, such as raw feature values and their complex embeddings – into high-level concepts that are understandable to humans <span id="id4">[<a class="reference internal" href="../../bibliography/bibliography.html#id3" title="Kacper Sokol and Peter Flach. Interpretable representations in explainable AI: From theory to practice. arXiv preprint arXiv:2008.07007, 2020. URL: http://arxiv.org/abs/2008.07007.">Sokol and Flach, 2020</a>]</span>.
This mapping establishes an <em>interface</em> between a computer-readable encoding of a phenomenon (captured by the collected data) and cognitively digestible chunks of information, thus providing a medium suitable for conveying explanations.
IRs directly control the (perceived) complexity of the explanations built on top of them, define the question that these insights answer and restrict the explanation types that can effectively communicate this information – e.g., influence or importance of interpretable concepts, counterfactuals and what-if statements – making IR-based explainers highly flexible, versatile and appealing.</p>
<p>By customising the interpretable representation we can adjust the content and comprehensibility of the resulting explanations and tune them towards a particular <em>audience</em> and <em>application</em>.
The algorithmic process responsible for transforming data from their original domain into an interpretable representation is usually defined by a human and captured by a function <span class="math notranslate nohighlight">\(\IR : \mathcal{X} \mapsto \mathcal{X}^\prime\)</span> whose inverse is denoted by <span class="math notranslate nohighlight">\(\IR^{-1} : \mathcal{X}^\prime \mapsto \mathcal{X}\)</span>.
Uniquely for tabular data, however, an interpretable representation can be learnt as part of the <em><a class="reference internal" href="#text-meta-explainers-surrogates-overview-surrogate-components-explanation-generation"><span class="std std-ref">explanation generation</span></a></em> step, depending on the choice of the surrogate model <span id="id5">[<a class="reference internal" href="../../bibliography/bibliography.html#id3" title="Kacper Sokol and Peter Flach. Interpretable representations in explainable AI: From theory to practice. arXiv preprint arXiv:2008.07007, 2020. URL: http://arxiv.org/abs/2008.07007.">Sokol and Flach, 2020</a>; <a class="reference internal" href="../../bibliography/bibliography.html#id16" title="Kacper Sokol, Alexander Hepburn, Raul Santos-Rodriguez, and Peter Flach. bLIMEy: Surrogate prediction explanations beyond LIME. 2019 Workshop on Human-Centric Machine Learning (HCML 2019) at the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada, 2019. arXiv preprint arXiv:1910.13016. URL: https://arxiv.org/abs/1910.13016.">Sokol <em>et al.</em>, 2019</a>]</span>.</p>
<p>While the operationalisation of interpretable representations vary for different data types – <a class="reference internal" href="#text-meta-explainers-surrogates-overview-data-types-tabular"><span class="std std-ref">tabular</span></a>, <a class="reference internal" href="#text-meta-explainers-surrogates-overview-data-types-image"><span class="std std-ref">image</span></a> and <a class="reference internal" href="#text-meta-explainers-surrogates-overview-data-types-text"><span class="std std-ref">text</span></a> – their machine representation is usually consistent: a binary vector indicating presence (<em>fact</em> denoted by <span class="math notranslate nohighlight">\(1\)</span>) or absence (<em>foil</em> denoted by <span class="math notranslate nohighlight">\(0\)</span>) of certain human-understandable concepts for a selected data point, i.e., <span class="math notranslate nohighlight">\(\mathcal{X}^\prime \equiv \{0, 1\}^{d^\prime}\)</span> where <span class="math notranslate nohighlight">\(d^\prime\)</span> is the number of binary interpretable features.
The IRs of <span class="xref std std-ref">image</span> and <a class="reference internal" href="text.html#text-meta-explainers-surrogates-text-interpretable-representation"><span class="std std-ref">text</span></a> data are relatively intuitive and share many properties.
Images can be partitioned into non-overlapping segments called <a class="reference internal" href="../../preface/glossary.html#term-super-pixel"><span class="xref std std-term">super-pixels</span></a>, each one corresponding to an object of interest or pieces thereof, which are then represented in the interpretable binary space as either preserved (i.e., original pixel values) or removed.
Similarly, text can be split into tokens that may encode individual words (e.g., a <a class="reference internal" href="../../preface/glossary.html#term-bag-of-words"><span class="xref std std-term">bag of words</span></a>), their stems or collections of words (that are not necessarily adjacent), the presence or absence of which is given by the IR.
<a class="reference internal" href="tabular/interpretable_representation.html#text-meta-explainers-surrogates-tabular-interpretable-representation"><span class="std std-ref">Tabular</span></a> data are more problematic since, first, numerical attributes need to be discretised to create a hyper-rectangle partition of the feature space – capturing meaningful patterns, e.g., people belonging to different age groups – followed by a binarisation procedure that for each (now discrete) dimension records whether a data point is located within or outside of the hyper-rectangle selected to be explained.
Some popular techniques are edge-based <em>super-pixel segmentation</em> for images, e.g., via quick shift <span id="id6">[<a class="reference internal" href="../../bibliography/bibliography.html#id19" title="Andrea Vedaldi and Stefano Soatto. Quick shift and kernel methods for mode seeking. In European conference on computer vision, 705–718. Springer, 2008.">Vedaldi and Soatto, 2008</a>]</span>; whitespace-based <em>tokenisation</em> for text; and <em>quantile discretisation</em> or <em>decision tree-based partition</em> for numerical features of tabular data <span id="id7">[<a class="reference internal" href="../../bibliography/bibliography.html#id12" title="Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “Why should I trust you?”: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, 1135–1144. 2016.">Ribeiro <em>et al.</em>, 2016</a>]</span>.</p>
<p>These representation changes facilitate explainability of sensory data and allow to retrofit the resulting explainers into pre-existing (black-box) ML models.
While using an interpretable representation is required for the text and image domains, it is not mandatory – albeit helpful – for explaining tabular data.
IRs of text and images are reasonably easy to generate <em>automatically</em> and (when configured correctly) the meaning of the resulting explanations is relatively comprehensible to a lay audience – a characteristic that is not necessarily true of tabular data because of the unintuitive process employed to generate their IRs.
High dimensionality of raw data does not impair legibility of image and text interpretable representations, which is the case for tabular data where we are generally confined to three dimensions given the inherent spatio-temporal limitations of our visual apparatus.
Note that dimensionality reduction for images and text is unnecessary or even harmful; removal of super-pixels is an ill-defined procedure that results in “blank spaces”, whereas discarding stop words and punctuation marks as well as word transformations can be considered as pruning steps that should be incorporated directly into the interpretable representation composition function and executed prior to tokenisation.
Dropping features in either the original or interpretable representation of tabular data, on the other hand, reduces the size of the resulting explanation.</p>
<p>Specifying the foil of an interpretable representation – i.e., the operation linked to switching off a component of the IR by setting its binary value to <span class="math notranslate nohighlight">\(0\)</span> – may not always be straightforward, practical or even (computationally) feasible in certain domains <span id="id8">[<a class="reference internal" href="../../bibliography/bibliography.html#id20" title="Brent Mittelstadt, Chris Russell, and Sandra Wachter. Explaining explanations in AI. In Proceedings of the conference on fairness, accountability, and transparency, 279–288. 2019.">Mittelstadt <em>et al.</em>, 2019</a>]</span>, requiring a problem-specific <em><a class="reference internal" href="../../preface/glossary.html#term-information-removal-proxy"><span class="xref std std-term">information removal proxy</span></a></em>.
Most data-driven models compatible with tabular and image data cannot predict incomplete instances forcing the IR to rely on such a proxy, e.g., <span class="xref std std-ref">colour-based segment occlusion for images</span> or <a class="reference internal" href="tabular/interpretable_representation.html#text-meta-explainers-surrogates-tabular-interpretable-representation"><span class="std std-ref">random hyper-rectangle allocation for tabular data</span></a>.
Predictive models that deal with text do not presuppose an input of fixed size, allowing for its pieces to be directly discarded.</p>
<p>Since parameterisation and configuration of interpretable representations, as well as any information removal proxy, can lead to biased and untrustworthy explanations, use case-specific ablation studies may be necessary <span id="id9">[<a class="reference internal" href="../../bibliography/bibliography.html#id3" title="Kacper Sokol and Peter Flach. Interpretable representations in explainable AI: From theory to practice. arXiv preprint arXiv:2008.07007, 2020. URL: http://arxiv.org/abs/2008.07007.">Sokol and Flach, 2020</a>]</span>.
Consider the possible influence of different text tokenisation strategies; granularity of super-pixel segmentation and the occlusion colour used for information removal; or discretisation of numerical features.
IRs may also impose implicit assumptions such as the <em>locality</em> of an explanation – e.g., image IRs operate within the remit of a single picture – which can affect the <a class="reference internal" href="../../preface/glossary.html#term-completeness"><span class="xref std std-term">completeness</span></a> of an explanation.
If the transformation between the original and interpretable domains <span class="math notranslate nohighlight">\(\IR\)</span>, or its inverse <span class="math notranslate nohighlight">\(\IR^{-1}\)</span>, is <em>stochastic</em>, this procedure is likely to introduce unnecessary randomness, contributing to explanation volatility, reducing their <a class="reference internal" href="../../preface/glossary.html#term-fidelity"><span class="xref std std-term">fidelity</span></a> and <a class="reference internal" href="../../preface/glossary.html#term-soundness"><span class="xref std std-term">soundness</span></a>, and harming their <em>veracity</em> <span id="id10">[<a class="reference internal" href="../../bibliography/bibliography.html#id18" title="Kacper Sokol and Peter Flach. Explainability fact sheets: A framework for systematic assessment of explainable approaches. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 56–67. 2020.">Sokol and Flach, 2020</a>; <a class="reference internal" href="../../bibliography/bibliography.html#id16" title="Kacper Sokol, Alexander Hepburn, Raul Santos-Rodriguez, and Peter Flach. bLIMEy: Surrogate prediction explanations beyond LIME. 2019 Workshop on Human-Centric Machine Learning (HCML 2019) at the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada, 2019. arXiv preprint arXiv:1910.13016. URL: https://arxiv.org/abs/1910.13016.">Sokol <em>et al.</em>, 2019</a>]</span>.
The operationalisation of interpretable representations as well as the information removal proxies they employ should therefore be well-understood, domain-aware and deterministic.</p>
</section>
<section id="data-sampling">
<h3>Data Sampling<a class="headerlink" href="#data-sampling" title="Link to this heading">#</a></h3>
<p>Data sampling allows to capture the behaviour of a predictive model in a desired subspace.
To this end, a data sample is generated and predicted by the explained model, offering a granular insight into its decision surface.
This procedure can either be carried out in the original <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> or interpretable <span class="math notranslate nohighlight">\(\mathcal{X}^\prime\)</span> representation.
The latter approach helps to avoid ill-defined sampling that would otherwise be required in the text and image domains;
tabular data can be sampled in either representation albeit with certain caveats.</p>
<p>Sampling in the interpretable domain is computationally-efficient since <span class="math notranslate nohighlight">\(\mathcal{X}^\prime \equiv \{ 0, 1 \}^{d^\prime}\)</span> is a binary space.
Doing so, however, requires us to transform the sample back to the original representation <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> since it needs to be predicted by the explained model.
If the <span class="math notranslate nohighlight">\(IR^{-1} : \mathcal{X}^\prime \mapsto \mathcal{X}\)</span> procedure is <em>non-deterministic</em>, this step may introduce randomness into the explanations, thus decreasing their stability and reliability.
Additionally, any assumptions pertinent to the underlying interpretable representation are inherited by the sampling procedure.
While this step decides the <em><a class="reference internal" href="../../preface/glossary.html#term-scope"><span class="xref std std-term">scope</span></a></em> – local, cohort or global – of the explanation and should yield a sample that is representative of the region of interest, if data are drawn from an (implicitly) local IR, this limitation will apply here as well.</p>
<p>The size and distribution of the sample within the designated area are also important to explanation coverage.
Generating an <em>exhaustive</em> sample may not be possible in the original domain, requiring us to draw a <em>random</em> collection of data points.
This procedure is nonetheless detrimental to the stability and fidelity of explanations.
The same is not necessarily true of sampling in a binary interpretable representation.
Since their dimensionality <span class="math notranslate nohighlight">\(d^\prime\)</span> is usually low to reduce explanation complexity – the explainee is presented with fewer factors – producing a <em>complete</em> sample, which has <span class="math notranslate nohighlight">\(| \mathcal{X}^\prime | \equiv | \{ 0, 1 \}^{d^\prime} | = 2^{d^\prime}\)</span> unique elements, is feasible.
For example, working with <span class="math notranslate nohighlight">\(d^\prime = 11\)</span> interpretable concepts yields an IR whose cardinality is <span class="math notranslate nohighlight">\(| \mathcal{X}^\prime | \equiv | \{ 0, 1 \}^{11} | = 2^{11} = 2,048\)</span>.</p>
<p>Another important aspect of sampling is the <em>diversity</em> of generated data.
Given that the surrogate model ought to approximate the decision boundary of the explained model in the region of region of interest, ensuring that the sample spans at least one decision boundary is crucial.
Sampling should therefore discover a wide spectrum of probabilities for probabilistic models, the range of interest for numerical output of regressors, and multiple classes (at lest two) for crisp classifiers.</p>
</section>
<section id="explanation-generation">
<span id="text-meta-explainers-surrogates-overview-surrogate-components-explanation-generation"></span><h3>Explanation Generation<a class="headerlink" href="#explanation-generation" title="Link to this heading">#</a></h3>
<p>Explanatory insights are extracted from an inherently transparent model fitted to the sampled data, usually using their interpretable representation;
relying on the original representation is also possible for tabular data.
The surrogate model targets the predictions of this sample output by the explained model – numbers for regression, probabilities for probabilistic modelling and classes for crisp classification as explained earlier.</p>
<p>Additional processing steps can be introduced here as well.
<em>Dimensionality reduction</em> – predominantly used with either the original or interpretable representation of tabular data – can be applied to select a subset of (raw or interpretable) features before showing them to the explainee.
Introducing sparsity can improve accessibility and comprehensibility of explanatory insights.
The <em>scope</em> of the explanation can be further fine-tuned by weighting the sample during surrogate model training, with the weights calculated in either of the representations: original or interpretable.
One way to achieve that – specifically for local explanations of a selected data point – is by calculating the proximity of each sampled data point to the explained instance using a distance transformed into similarity through a kernel, as described earlier.
The choice of the distance metric and kernel as well as their parameterisation should depend on the problem and data at hand.</p>
<p>Any implicit or explicit assumptions pertinent to the interpretable representation should be rëevaluated when selecting the surrogate model type.
For example, consider a <em>sensitivity analysis</em> setting where coefficients of a surrogate linear model quantify the positive or negative influence of IR components (more precisely, information that they encode) on a specific prediction output by the explained model for a selected instance.
(Recall that the sampled data capture a collection of points with a random subset of IR elements “removed”, the effect of which procedure is recorded by predicting these instances with the explained model.)
Focusing on image data, the binary IR features are based on super-pixels, which are correlated given the colour continuity found in the underlying image spilling across adjacent segments.
This clearly violates the feature independence assumptions imposed by linear models.
When using this family of models as surrogates another implicit requirement is for all of the underlying features to be normalised to the same range, which makes the coefficients directly comparable.
While this is true for <em>binary</em> IRs, deploying such a surrogate on raw tabular data requires an additional step that scales the features.</p>
</section>
</section>
<section id="surrogates-across-data-types">
<span id="text-meta-explainers-surrogates-overview-data-types"></span><h2><span class="section-number">1.1.3. </span>Surrogates Across Data Types<a class="headerlink" href="#surrogates-across-data-types" title="Link to this heading">#</a></h2>
<p>Given the use of <em>interpretable representations</em> surrogate explainers can be applied to various data domains: tabular, image and text.
Each of them receives a dedicated section in this book, but here we overview them briefly for completeness of the surrogate overview.</p>
<p>The added benefit of this setup is that explanations look the same regardless of the data</p>
<section id="text">
<span id="text-meta-explainers-surrogates-overview-data-types-text"></span><h3>Text<a class="headerlink" href="#text" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="text.html#text-meta-explainers-surrogates-text"><span class="std std-numref">Overview (§1.2)</span></a>
<a class="reference internal" href="text.html#text-meta-explainers-surrogates-text-interpretable-representation"><span class="std std-numref">Interpretable Representation (§1.2.1)</span></a>
<a class="reference internal" href="text.html#text-meta-explainers-surrogates-text-data-sampling"><span class="std std-numref">Data Sampling (§1.2.2)</span></a>
<a class="reference internal" href="text.html#text-meta-explainers-surrogates-text-explanation-generation"><span class="std std-numref">Explanation Generation (§1.2.3)</span></a></p>
<p><strong><a class="reference internal" href="text.html#text-meta-explainers-surrogates-text-interpretable-representation"><span class="std std-ref">Interpretable Representation.</span></a></strong></p>
</section>
<section id="images">
<span id="text-meta-explainers-surrogates-overview-data-types-image"></span><h3>Images<a class="headerlink" href="#images" title="Link to this heading">#</a></h3>
<p><strong><span class="xref std std-ref">Interpretable Representation.</span></strong></p>
<p><a class="reference internal" href="image.html#text-meta-explainers-surrogates-image"><span class="std std-numref">Overview (§1.3)</span></a>
<code class="xref std std-numref docutils literal notranslate"><span class="pre">Interpretable</span> <span class="pre">Representation</span> <span class="pre">(§%s)</span></code>
<code class="xref std std-numref docutils literal notranslate"><span class="pre">Data</span> <span class="pre">Sampling</span> <span class="pre">(§%s)</span></code>
<code class="xref std std-numref docutils literal notranslate"><span class="pre">Explanation</span> <span class="pre">Generation</span> <span class="pre">(§%s)</span></code></p>
</section>
<section id="tabular-data">
<span id="text-meta-explainers-surrogates-overview-data-types-tabular"></span><h3>Tabular Data<a class="headerlink" href="#tabular-data" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="tabular/index.html#text-meta-explainers-surrogates-tabular"><span class="std std-numref">Overview (§1.4)</span></a>
<a class="reference internal" href="tabular/interpretable_representation.html#text-meta-explainers-surrogates-tabular-interpretable-representation"><span class="std std-numref">Interpretable Representation (§1.4.1)</span></a>
<a class="reference internal" href="tabular/data_sampling.html#text-meta-explainers-surrogates-tabular-data-sampling"><span class="std std-numref">Data Sampling (§1.4.2)</span></a>
<a class="reference internal" href="tabular/explanation_generation.html#text-meta-explainers-surrogates-tabular-explanation-generation"><span class="std std-numref">Explanation Generation (§1.4.3)</span></a></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The official implementation of LIME[^lime_implementation] samples tabular data from the interpretable domain.
This has two major consequences.</p>
<ol class="arabic simple">
<li><p>The sampling is actually global, with locality introduced only via instance weighting.</p></li>
<li><p>Since the sampled instances need to be restored back to the original representation to xxx, etc.</p></li>
</ol>
</div>
<p>each component brings in assumptions and limitation that permeate to affect the resulting explanations</p>
<p>TODO complexity for tree
TODO complexity for line</p>
</section>
</section>
<section id="explanation-examples">
<span id="text-meta-explainers-surrogates-overview-explanations"></span><h2><span class="section-number">1.1.4. </span>Explanation Examples<a class="headerlink" href="#explanation-examples" title="Link to this heading">#</a></h2>
<div class="output text_html"><table border="1" class="dataframe" style="margin-left: auto; margin-right: auto; margin-bottom: 1em;">
    <thead>
        <tr>
            <th><span class="math">\(\mathring{c}\)</span></th>
            <th><span class="math">\(\mathring{b}\)</span></th>
            <th><span class="math">\(\mathring{a}\)</span></th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>1.00</td>
            <td>2.00</td>
            <td>3.00</td>
        </tr>
        <tr>
            <td>1.00</td>
            <td>2.00</td>
            <td>3.00</td>
        </tr>
        <tr>
            <td>1.00</td>
            <td>2.00</td>
            <td>3.00</td>
        </tr>
        <tr>
            <td>1.00</td>
            <td>2.00</td>
            <td>3.00</td>
        </tr>
    </tbody>
</table></div><p>varying scope</p>
<p>concept influence</p>
<p>counterfactuals, e.g., with trees cite LIMEtree</p>
<section id="linear">
<h3>Linear<a class="headerlink" href="#linear" title="Link to this heading">#</a></h3>
</section>
<section id="tree">
<h3>Tree<a class="headerlink" href="#tree" title="Link to this heading">#</a></h3>
</section>
<section id="multi-output-tree">
<span id="text-meta-explainers-surrogates-overview-explanations-multi-output-trees"></span><h3>Multi-output Tree<a class="headerlink" href="#multi-output-tree" title="Link to this heading">#</a></h3>
</section>
</section>
<section id="evaluation-strategies">
<h2><span class="section-number">1.1.5. </span>Evaluation Strategies<a class="headerlink" href="#evaluation-strategies" title="Link to this heading">#</a></h2>
<section id="id11">
<h3>Interpretable Representations<a class="headerlink" href="#id11" title="Link to this heading">#</a></h3>
</section>
<section id="id12">
<h3>Data Sampling<a class="headerlink" href="#id12" title="Link to this heading">#</a></h3>
</section>
<section id="id13">
<h3>Explanation Generation<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
</section>
<section id="end-to-end">
<h3>End-to-End<a class="headerlink" href="#end-to-end" title="Link to this heading">#</a></h3>
<p>Algorithmic evaluation of surrogates xxx
eval components
and fidelity of explanations</p>
<figure class="align-default" id="fig-surrogates-eval" style="width: 95%">
<img alt="../../../_images/a6ca7fe361a1bcf4c964e63faf5b3fad2cbe05b29a402396d02030e414a1783e.svg" src="../../../_images/a6ca7fe361a1bcf4c964e63faf5b3fad2cbe05b29a402396d02030e414a1783e.svg" />
<figcaption>
<p><span class="caption-number">Figure 1.3 </span><span class="caption-text">Overview of fidelity-based evaluation strategies for surrogates of <em>tabular</em> data, using the example of a <em>local</em>, <em>linear</em> explainer without an interpretable representation.
Panels (a) and (b) show <em>data-driven</em> evaluation approaches; the former for a data subspace and the latter for the entire data space (gray shading).
Panels (c) and (d) depict <em>model-driven</em> evaluation approaches; the former for the local (surrogate) decision boundary and the latter for the global (black-box) decision boundary.</span><a class="headerlink" href="#fig-surrogates-eval" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>different strategies for tabular data, images can only eb evaluated withing the scope of a single image or sentence.</p>
<p>intuitive visualisation for the local and global, model specific and data-specific visualisations from the thesis.</p>
</section>
</section>
<section id="caveats">
<h2><span class="section-number">1.1.6. </span>Caveats<a class="headerlink" href="#caveats" title="Link to this heading">#</a></h2>
<p>fidelity <span id="id14">[<a class="reference internal" href="../../bibliography/bibliography.html#id15" title="Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):206–215, 2019.">Rudin, 2019</a>]</span>
stochasticity: random sampling, operating on interpretable representation of tabular data</p>
</section>
<section id="additional-resources">
<span id="text-meta-explainers-surrogates-overview-literature"></span><h2><span class="section-number">1.1.7. </span>Additional Resources<a class="headerlink" href="#additional-resources" title="Link to this heading">#</a></h2>
<p>TODO</p>
<p>Materials</p>
<p>Do a quick overview of surrogate literature.</p>
<p>popular examples are treepan <span id="id15">[<a class="reference internal" href="../../bibliography/bibliography.html#id5" title="Mark Craven and Jude W Shavlik. Extracting tree-structured representations of trained networks. In Advances in neural information processing systems, 24–30. 1996.">Craven and Shavlik, 1996</a>]</span> and LIME <span id="id16">[<a class="reference internal" href="../../bibliography/bibliography.html#id12" title="Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “Why should I trust you?”: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, 1135–1144. 2016.">Ribeiro <em>et al.</em>, 2016</a>]</span>,
and a generalisation of LIME bLIMEy, which is modular meta-algorithm for building surrogates</p>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="scaling" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">1</a><span class="fn-bracket">]</span></span>
<p>The maximum component-wise squared difference between two probability vectors <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> – i.e., <span class="math notranslate nohighlight">\(\sum_i \left( \mathbf{a}_i - \mathbf{b}_i \right)^2\)</span> – is <span class="math notranslate nohighlight">\(2\)</span> when the entire probability mass is assigned to a different component of each vector, e.g., <span class="math notranslate nohighlight">\(\mathbf{a} = [0, 0, 1]\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b} = [1, 0, 0]\)</span>.</p>
</aside>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./book/meta_explainers/surrogates"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">1. </span>Surrogates</p>
      </div>
    </a>
    <a class="right-next"
       href="text.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">1.2. </span>Text Surrogates</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-surrogates">1.1.1. Building Surrogates</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimisation-objective">Optimisation Objective</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#complexity">Complexity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fidelity-loss-one-class">Fidelity Loss (One Class)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fidelity-loss-multiple-classes">Fidelity Loss (Multiple Classes)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fidelity-loss-scaling">Fidelity Loss Scaling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#surrogate-building-blocks">1.1.2. Surrogate Building Blocks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretable-representations">Interpretable Representations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-sampling">Data Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-generation">Explanation Generation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#surrogates-across-data-types">1.1.3. Surrogates Across Data Types</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text">Text</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#images">Images</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tabular-data">Tabular Data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-examples">1.1.4. Explanation Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear">Linear</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tree">Tree</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-output-tree">Multi-output Tree</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-strategies">1.1.5. Evaluation Strategies</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Interpretable Representations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Data Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Explanation Generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#end-to-end">End-to-End</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#caveats">1.1.6. Caveats</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">1.1.7. Additional Resources</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By <a href="mailto:kacper@xmlx.dev">Kacper Sokol</a> and <a href="mailto:team@xmlx.dev">XMLX Team</a> &ndash; <a href="https://xmlx.dev/">xmlx.dev</a>. <br> Book distributed under <a href="https://github.com/xmlx-dev/xml-book/blob/master/LICENCE">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Licence</a>. <br> Code distributed under <a href="https://github.com/xmlx-dev/xml-book/blob/master/LICENCE-code"> MIT Licence</a>.

</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2021–2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p> This book delivers a comprehensive outlook on the most popular explainability concepts in Machine Learning. It covers a range of theoretical and practical topics across different difficulty levels, including but not limited to: high-level overviews & introductory examples; mathematical foundations; algorithmic implementations; practical advice & real-life caveats; and success & failure case studies. </p>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>